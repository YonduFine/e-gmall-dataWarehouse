# 定义组件
a1.sources = r1
a1.channels = c1
a1.sinks = k1
# 配置sources
a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSource
a1.sources.r1.kafka.bootstrap.servers = bigdata101:9092,bigdata102:9092
a1.sources.r1.kafka.topics = topic_db
a1.sources.r1.kafka.consumer.group.id = kafka_hdfs_log
a1.sources.r1.batchSize = 1000
a1.sources.r1.batchDurationMillis = 1000
# 配置拦截器
a1.sources.r1.interceptors = i1
a1.sources.r1.interceptors.i1.type = com.leon.gamll.flume.interceptor.TimestampAndTableNameInterceptor$TBuild
# 配置channels
a1.channels.c1.type = file
a1.channels.c1.checkpointDir = /opt/module/flume-1.9.0/jobs/checkpoint/gmall_db
a1.channels.c1.useDualCheckpoints = false
a1.channels.c1.dataDirs = /opt/module/flume-1.9.0/datas/gmall_db
a1.channels.c1.maxFileSize = 2146435071
a1.channels.c1.capacity = 1000000
a1.channels.c1.keep-alive = 3
# 配置sinks
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = hdfs://bigdata101:9820/origin_data/gmall/db/%{tableName}_inc/%Y-%m-%d
a1.sinks.k1.hdfs.filePrefix = log
a1.sinks.k1.hdfs.round = false
# 控制文件滚动大小,解决小文件问题
a1.sinks.k1.hdfs.rollInterval = 20
a1.sinks.k1.hdfs.rollSize = 134217728
a1.sinks.k1.hdfs.rollCount = 0
# 控制文件存储类型
a1.sinks.k1.hdfs.fileType = CompressedStream
a1.sinks.k1.hdfs.codeC = gzip
# 组装
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1