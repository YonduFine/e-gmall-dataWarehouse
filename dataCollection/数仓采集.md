# 一、数据采集
## 1.1 日志数据采集

运行节点：bigdata101、bigdata102

flume配置文件: `file-to-kafka.conf`内容:
```
# 定义
a1.sources = r1
a1.channels = c1
# 配置sources
a1.sources.r1.type = TAILDIR
a1.sources.r1.filegroups = f1
a1.sources.r1.filegroups.f1 = /opt/module/applog/log/app.*
a1.sources.r1.positionFile = /opt/module/flume-1.9.0/jobs/position/log_file_position.json
a1.sources.r1.maxBatchCount = 1000
# 添加数据格式校验拦截器
a1.sources.r1.interceptors = i1
a1.sources.r1.interceptors.i1.type = com.leon.gamll.flume.interceptor.ETLInterceptor$ETLBuilder
# 配置channels
a1.channels.c1.type = org.apache.flume.channel.kafka.KafkaChannel
a1.channels.c1.kafka.bootstrap.servers = bigdata101:9092,bigdata102:9092
a1.channels.c1.kafka.topic = topic_log
a1.channels.c1.kafka.consumer.group.id = file_kafka_g1
a1.channels.c1.parseAsFlumeEvent = false
# 组装
a1.sources.r1.channels = c1
```
> 注：拦截器代码见模块`flume-interceptor`下`com.leon.gamll.flume.interceptor.ETLInterceptor.class`
> 
<b>测试</b>

测试脚本

需要的进程：`zookeeper` `Kafka` `日志生成器`

启动顺序：
    <ol>
    <li>启动`lg.sh`日志生成器，命令：`lg.sh start` </li>
    <li>启动一个Kafka消费者，命令：`kafka-console-consumer.sh --bootstrap-server bigdata101:9092 --topic topic_log`</li>
    <li>启动flume，命令：`bin/flume-ng agent -n a1 -c /conf -f jobs/gmall/file_to_kafka.conf -Dflume.root.logger=info,console`</li>
    </ol>

<b>配置群集脚本</b>

需要的工作:

将配置信息同步至bigdata102,命令：`cd /opt/module/flume-1.9.0/jobs/gmall`, `xysync.sh file_to_kafka.conf`

上游采集通道群起脚本

```shell
#! /bin/bash
if [ $# -lt 1 ]
then
    echo "参数不能为空! 请输入start | stop"
    exit 
fi

flumeconf=file_to_kafka.conf

case $1 in
"start")
  for host in bigdata101 bigdata102
  do
    ssh $host "nohup /opt/module/flume-1.9.0/bin/flume-ng agent -n a1 -c /conf -f /opt/module/flume-1.9.0/jobs/gmall/file_to_kafka.conf >/dev/null 2>&1 &"
    echo "$host 上游日志采集服务启动"
  done
;;
"stop")
  for host in bigdata101 bigdata102
  do
    ssh $host "ps -ef | grep -v 'grep' | grep -i $flumeconf | awk '{print \$2}' | xargs -n1 kill -9"
    echo "$host 上游日志采集服务已停止"
  done
;;
*)
    echo "输入的参数有误! 请输入 start | stop"
;;
esac
```

## 1.2 业务数据采集
### 1.2.1 增量数据同步
运行节点：bigdata101

采用工具：maxwell [version:1.2.92]

前置条件 1：MySQL开启binlog
```shell
sudo vim /etc/my.cnf
```
```properties
# 数据库id
server-id=1
# 启动bin-log,该参数的值会作为binlog的文件名
log-bin=mysql-bin
# binlog类型，Maxwell要求为row类型
binlog_format=row
# 启动binlog的数据库,需要根据实际情况作出修改
binlog-do-db=gmall
```
编辑后需重启MySQL服务
前置条件 2：MySQL配置Maxwell用户
```sql
--创建数据库
mysql> create database maxwell;
--调整MySQL密码级别
mysql> set global validate_password_policy=0;
mysql> set global validate_password_length=4;
--创建Maxwell用户并赋予权限
mysql> CREATE USER 'maxwell'@'%' IDENTIFIED BY 'maxwell';
mysql> GRANT ALL ON maxwell.* TO 'maxwell'@'%';
mysql> GRANT SELECT, REPLICATION CLIENT, REPLICATION SLAVE ON *.* TO 'maxwell'@'%';
```
Maxwell配置信息：
```shell
cd /opt/module/maxwell
mv config.properties.template config.properties
vim config.properties
```
编辑以下配置：
```properties
producer=kafka
kafka.bootstrap.servers=bigdata101:9092,bigdata102:9092
kafka_topic=topic_db

# mysql login info
host=bigdata101
user=maxwell
password=maxwell
jdbc_options=useSSL=false&serverTimezone=Asia/Shanghai
```
<b>测试</b>
    <ol>
    <li>需要的服务: `zookeeper` `kafka`</li>
    <li>启动maxwell: `/opt/module/maxwell/bin/maxwell --config /opt/module/maxwell/config.properties --daemon`</li>
    <li>启动一个Kafka消费者: `kafka-console-consumer.sh --bootstrap-server bigdata101:9092 --topic topic_db`</li>
    <li>启动业务日志生成模块: `cd /opt/module/db_log; java -jar gmall2020-mock-db-2021-11-14.jar`</li>
    <li>观察Kafka消费者是否有数据输出</li>
    </ol>
## 1.3 数据同步
### 1.3.1 日志数据同步
运行节点：bigdata103

flume配置文件：`kafka_to_hdfs_log.conf`内容：
```
# 定义组件
a1.sources = r1
a1.channels = c1
a1.sinks = k1
# 配置sources
a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSource
a1.sources.r1.kafka.bootstrap.servers = bigdata101:9092,bigdata102:9092
a1.sources.r1.kafka.topics = topic_log
a1.sources.r1.kafka.consumer.group.id = kafka_hdfs_log
a1.sources.r1.batchSize = 1000
a1.sources.r1.batchDurationMillis = 1000
# 配置拦截器
a1.sources.r1.interceptors = i1
a1.sources.r1.interceptors.i1.type = com.leon.gamll.flume.interceptor.TimestampInterceptor$TsBuilder
# 配置channels
a1.channels.c1.type = file
a1.channels.c1.checkpointDir = /opt/module/flume-1.9.0/jobs/checkpoint/gmall_log
a1.channels.c1.useDualCheckpoints = false
a1.channels.c1.dataDirs = /opt/module/flume-1.9.0/datas/gmall_log
a1.channels.c1.maxFileSize = 2146435071
a1.channels.c1.capacity = 1000000
a1.channels.c1.keep-alive = 3
# 配置sinks
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = hdfs://bigdata101:9820/origin_data/gmall/log/topic_log/%Y-%m-%d
a1.sinks.k1.hdfs.filePrefix = log
a1.sinks.k1.hdfs.round = false
# 控制文件滚动大小,解决小文件问题
a1.sinks.k1.hdfs.rollInterval = 20
a1.sinks.k1.hdfs.rollSize = 134217728
a1.sinks.k1.hdfs.rollCount = 0
# 控制文件存储类型
a1.sinks.k1.hdfs.fileType = CompressedStream
a1.sinks.k1.hdfs.codeC = gzip
# 组装
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
```
> 注: 拦截器代码见模块`flume-interceptor`下`com.leon.gamll.flume.interceptor.TimestampInterceptor`

<b>测试</b>
    <ol>
    <li>需要的服务 `zookeeper` `Kafka` `hdfs`</li>
    <li>启动上游数据采集服务(bigdata101) `f1_log.sh start`</li>
    <li>启动数据同步flume(bigdata103) 

`cd /opt/module/flume-1.9.0; ./bin/flume-ng agent -n a1 -c /conf -f jobs/gmall/kafka_to_hdfs_log.conf -Dflume.root.logger=info,console`</li>
    <li>启动日志生成模块(bigdata101)`lg.sh start`</li>
    <li>访问HDFS对应目录，观察目录下是否有文件生成</li>
    </ol>

下游数据采集脚本

运行节点：bigdata101

`f2_log.sh`内容：
```shell
#! /bin/bash
if [ $# -lt 1 ]
then
    echo "参数不能为空! 请输入start | stop"
    exit 
fi

flumeconf=kafka_to_hdfs_log.conf
host=bigdata103

case $1 in
"start")
    ssh $host "nohup /opt/module/flume-1.9.0/bin/flume-ng agent -n a1 -c /conf -f /opt/module/flume-1.9.0/jobs/gmall/$flumeconf >/dev/null 2>&1 &"
    echo "$host 下游日志采集服务启动"
;;
"stop")
    ssh $host "ps -ef | grep -v 'grep' | grep -i $flumeconf | awk '{print \$2}' | xargs -n1 kill -9"
    echo "$host 下游日志采集服务已停止"
;;
*)
    echo "输入的参数有误! 请输入 start | stop"
;;
esac
```

### 1.3.2 业务数据同步
#### 全量数据同步
采用工具: DataX

在`~/bin`目录下编辑以下配置文件

生成全量表dataX的json配置文件:
`gen_import_config.py`
```python
# ecoding=utf-8
import json
import getopt
import os
import sys
import MySQLdb

#MySQL相关配置，需根据实际情况作出修改
mysql_host = "bigdata101"
mysql_port = "3306"
mysql_user = "root"
mysql_passwd = "123456"

#HDFS NameNode相关配置，需根据实际情况作出修改
hdfs_nn_host = "bigdata101"
hdfs_nn_port = "9820"

#生成配置文件的目标路径，可根据实际情况作出修改
output_path = "/opt/module/datax/job/import"


def get_connection():
    return MySQLdb.connect(host=mysql_host, port=int(mysql_port), user=mysql_user, passwd=mysql_passwd)


def get_mysql_meta(database, table):
    connection = get_connection()
    cursor = connection.cursor()
    sql = "SELECT COLUMN_NAME,DATA_TYPE from information_schema.COLUMNS WHERE TABLE_SCHEMA=%s AND TABLE_NAME=%s ORDER BY ORDINAL_POSITION"
    cursor.execute(sql, [database, table])
    fetchall = cursor.fetchall()
    cursor.close()
    connection.close()
    return fetchall


def get_mysql_columns(database, table):
    return map(lambda x: x[0], get_mysql_meta(database, table))


def get_hive_columns(database, table):
    def type_mapping(mysql_type):
        mappings = {
            "bigint": "bigint",
            "int": "bigint",
            "smallint": "bigint",
            "tinyint": "bigint",
            "decimal": "string",
            "double": "double",
            "float": "float",
            "binary": "string",
            "char": "string",
            "varchar": "string",
            "datetime": "string",
            "time": "string",
            "timestamp": "string",
            "date": "string",
            "text": "string"
        }
        return mappings[mysql_type]

    meta = get_mysql_meta(database, table)
    return map(lambda x: {"name": x[0], "type": type_mapping(x[1].lower())}, meta)


def generate_json(source_database, source_table):
    job = {
        "job": {
            "setting": {
                "speed": {
                    "channel": 3
                },
                "errorLimit": {
                    "record": 0,
                    "percentage": 0.02
                }
            },
            "content": [{
                "reader": {
                    "name": "mysqlreader",
                    "parameter": {
                        "username": mysql_user,
                        "password": mysql_passwd,
                        "column": get_mysql_columns(source_database, source_table),
                        "splitPk": "",
                        "connection": [{
                            "table": [source_table],
                            "jdbcUrl": ["jdbc:mysql://" + mysql_host + ":" + mysql_port + "/" + source_database]
                        }]
                    }
                },
                "writer": {
                    "name": "hdfswriter",
                    "parameter": {
                        "defaultFS": "hdfs://" + hdfs_nn_host + ":" + hdfs_nn_port,
                        "fileType": "text",
                        "path": "${targetdir}",
                        "fileName": source_table,
                        "column": get_hive_columns(source_database, source_table),
                        "writeMode": "append",
                        "fieldDelimiter": "\t",
                        "compress": "gzip"
                    }
                }
            }]
        }
    }
    if not os.path.exists(output_path):
        os.makedirs(output_path)
    with open(os.path.join(output_path, ".".join([source_database, source_table, "json"])), "w") as f:
        json.dump(job, f)


def main(args):
    source_database = ""
    source_table = ""

    options, arguments = getopt.getopt(args, '-d:-t:', ['sourcedb=', 'sourcetbl='])
    for opt_name, opt_value in options:
        if opt_name in ('-d', '--sourcedb'):
            source_database = opt_value
        if opt_name in ('-t', '--sourcetbl'):
            source_table = opt_value

    generate_json(source_database, source_table)


if __name__ == '__main__':
    main(sys.argv[1:])
```

配置生成json配置文件的shell脚本: 
`gen_import_config.sh`
```shell
#!/bin/bash

python ~/bin/gen_import_config.py -d gmall -t activity_info
python ~/bin/gen_import_config.py -d gmall -t activity_rule
python ~/bin/gen_import_config.py -d gmall -t base_category1
python ~/bin/gen_import_config.py -d gmall -t base_category2
python ~/bin/gen_import_config.py -d gmall -t base_category3
python ~/bin/gen_import_config.py -d gmall -t base_dic
python ~/bin/gen_import_config.py -d gmall -t base_province
python ~/bin/gen_import_config.py -d gmall -t base_region
python ~/bin/gen_import_config.py -d gmall -t base_trademark
python ~/bin/gen_import_config.py -d gmall -t cart_info
python ~/bin/gen_import_config.py -d gmall -t coupon_info
python ~/bin/gen_import_config.py -d gmall -t sku_attr_value
python ~/bin/gen_import_config.py -d gmall -t sku_info
python ~/bin/gen_import_config.py -d gmall -t sku_sale_attr_value
python ~/bin/gen_import_config.py -d gmall -t spu_info
```

测试：

前置条件：启动hdfs,业务数据库有数据
    <ol>
    <li>JSON模板脚本参数说明: `python gen_import_config.py -d database -t table` -d 数据名 -t 表名</li>
    <li>执行脚本生成每张表的dataX的json文件:`gen_import_config.sh`</li>
    <li>切换目录到`/opt/module/data/job/import`查看生成的json文件</li>
    <li>在HDFS上创建目录`hadoop fs -mkdir /origin_data/gmall/db/activity_info_full/2020-06-14`</li>
    <li>切换目录至`/opt/moudle/datax`并执行dataX同步命令：<br>`python bin/datax.py -p"-Dtargetdir=/origin_data/gmall/db/activity_info_full/2020-06-14" /job/import/gmall.activity_info.json`</li>
    <li>前往HDFS的对应目录查看文件，可使用shell命令查看文件内容：`hdfs dfs -cat file | zcat`</li>
    </ol> 

编辑全量数据同步脚本`mysql_to_hdfs_full.sh`:
```shell
#!/bin/bash

DATAX_HOME=/opt/module/datax

# 校验参数
if [ $# -lt 1 ]
then
  echo "参数输不能为空!"
  echo "参数1：要同步的表名，同步所有表请输入 all ; 参数2：可选参数 同步数据的日期"
fi

# 如果传入日期则do_date等于传入的日期，否则等于前一天日期
if [ -n "$2" ] ;then
    do_date=$2
else
    do_date=`date -d "-1 day" +%F`
fi

#处理目标路径，此处的处理逻辑是，如果目标路径不存在，则创建；若存在，则清空，目的是保证同步任务可重复执行
handle_targetdir() {
  hadoop fs -test -e $1
  if [[ $? -eq 1 ]]; then
    echo "路径$1不存在，正在创建......"
    hadoop fs -mkdir -p $1
  else
    echo "路径$1已经存在"
    fs_count=$(hadoop fs -count $1)
    content_size=$(echo $fs_count | awk '{print $3}')
    if [[ $content_size -eq 0 ]]; then
      echo "路径$1为空"
    else
      echo "路径$1不为空，正在清空......"
      hadoop fs -rm -r -f $1/*
    fi
  fi
}

#数据同步
import_data() {
  datax_config=$1
  target_dir=$2

  handle_targetdir $target_dir
  python $DATAX_HOME/bin/datax.py -p"-Dtargetdir=$target_dir" $datax_config
}

case $1 in
"activity_info")
  import_data /opt/module/datax/job/import/gmall.activity_info.json /origin_data/gmall/db/activity_info_full/$do_date
  ;;
"activity_rule")
  import_data /opt/module/datax/job/import/gmall.activity_rule.json /origin_data/gmall/db/activity_rule_full/$do_date
  ;;
"base_category1")
  import_data /opt/module/datax/job/import/gmall.base_category1.json /origin_data/gmall/db/base_category1_full/$do_date
  ;;
"base_category2")
  import_data /opt/module/datax/job/import/gmall.base_category2.json /origin_data/gmall/db/base_category2_full/$do_date
  ;;
"base_category3")
  import_data /opt/module/datax/job/import/gmall.base_category3.json /origin_data/gmall/db/base_category3_full/$do_date
  ;;
"base_dic")
  import_data /opt/module/datax/job/import/gmall.base_dic.json /origin_data/gmall/db/base_dic_full/$do_date
  ;;
"base_province")
  import_data /opt/module/datax/job/import/gmall.base_province.json /origin_data/gmall/db/base_province_full/$do_date
  ;;
"base_region")
  import_data /opt/module/datax/job/import/gmall.base_region.json /origin_data/gmall/db/base_region_full/$do_date
  ;;
"base_trademark")
  import_data /opt/module/datax/job/import/gmall.base_trademark.json /origin_data/gmall/db/base_trademark_full/$do_date
  ;;
"cart_info")
  import_data /opt/module/datax/job/import/gmall.cart_info.json /origin_data/gmall/db/cart_info_full/$do_date
  ;;
"coupon_info")
  import_data /opt/module/datax/job/import/gmall.coupon_info.json /origin_data/gmall/db/coupon_info_full/$do_date
  ;;
"sku_attr_value")
  import_data /opt/module/datax/job/import/gmall.sku_attr_value.json /origin_data/gmall/db/sku_attr_value_full/$do_date
  ;;
"sku_info")
  import_data /opt/module/datax/job/import/gmall.sku_info.json /origin_data/gmall/db/sku_info_full/$do_date
  ;;
"sku_sale_attr_value")
  import_data /opt/module/datax/job/import/gmall.sku_sale_attr_value.json /origin_data/gmall/db/sku_sale_attr_value_full/$do_date
  ;;
"spu_info")
  import_data /opt/module/datax/job/import/gmall.spu_info.json /origin_data/gmall/db/spu_info_full/$do_date
  ;;
"all")
  import_data /opt/module/datax/job/import/gmall.activity_info.json /origin_data/gmall/db/activity_info_full/$do_date
  import_data /opt/module/datax/job/import/gmall.activity_rule.json /origin_data/gmall/db/activity_rule_full/$do_date
  import_data /opt/module/datax/job/import/gmall.base_category1.json /origin_data/gmall/db/base_category1_full/$do_date
  import_data /opt/module/datax/job/import/gmall.base_category2.json /origin_data/gmall/db/base_category2_full/$do_date
  import_data /opt/module/datax/job/import/gmall.base_category3.json /origin_data/gmall/db/base_category3_full/$do_date
  import_data /opt/module/datax/job/import/gmall.base_dic.json /origin_data/gmall/db/base_dic_full/$do_date
  import_data /opt/module/datax/job/import/gmall.base_province.json /origin_data/gmall/db/base_province_full/$do_date
  import_data /opt/module/datax/job/import/gmall.base_region.json /origin_data/gmall/db/base_region_full/$do_date
  import_data /opt/module/datax/job/import/gmall.base_trademark.json /origin_data/gmall/db/base_trademark_full/$do_date
  import_data /opt/module/datax/job/import/gmall.cart_info.json /origin_data/gmall/db/cart_info_full/$do_date
  import_data /opt/module/datax/job/import/gmall.coupon_info.json /origin_data/gmall/db/coupon_info_full/$do_date
  import_data /opt/module/datax/job/import/gmall.sku_attr_value.json /origin_data/gmall/db/sku_attr_value_full/$do_date
  import_data /opt/module/datax/job/import/gmall.sku_info.json /origin_data/gmall/db/sku_info_full/$do_date
  import_data /opt/module/datax/job/import/gmall.sku_sale_attr_value.json /origin_data/gmall/db/sku_sale_attr_value_full/$do_date
  import_data /opt/module/datax/job/import/gmall.spu_info.json /origin_data/gmall/db/spu_info_full/$do_date
  ;;
esac
```

#### 增量数据同步
同步内容: 使用Maxwell采集变化的业务日志信息

执行节点: bigdata103

flume采集配置文件 `kafka_to_hdfs_db.conf`
```
# 定义组件
a1.sources = r1
a1.channels = c1
a1.sinks = k1
# 配置sources
a1.sources.r1.type = org.apache.flume.source.kafka.KafkaSource
a1.sources.r1.kafka.bootstrap.servers = bigdata101:9092,bigdata102:9092
a1.sources.r1.kafka.topics = topic_db
a1.sources.r1.kafka.consumer.group.id = kafka_hdfs_log
a1.sources.r1.batchSize = 1000
a1.sources.r1.batchDurationMillis = 1000
# 配置拦截器
a1.sources.r1.interceptors = i1
a1.sources.r1.interceptors.i1.type = com.leon.gamll.flume.interceptor.TimestampAndTableNameInterceptor$TBuild
# 配置channels
a1.channels.c1.type = file
a1.channels.c1.checkpointDir = /opt/module/flume-1.9.0/jobs/checkpoint/gmall_db
a1.channels.c1.useDualCheckpoints = false
a1.channels.c1.dataDirs = /opt/module/flume-1.9.0/datas/gmall_db
a1.channels.c1.maxFileSize = 2146435071
a1.channels.c1.capacity = 1000000
a1.channels.c1.keep-alive = 3
# 配置sinks
a1.sinks.k1.type = hdfs
a1.sinks.k1.hdfs.path = hdfs://bigdata101:9820/origin_data/gmall/db/%{tableName}_inc/%Y-%m-%d
a1.sinks.k1.hdfs.filePrefix = log
a1.sinks.k1.hdfs.round = false
# 控制文件滚动大小,解决小文件问题
a1.sinks.k1.hdfs.rollInterval = 20
a1.sinks.k1.hdfs.rollSize = 134217728
a1.sinks.k1.hdfs.rollCount = 0
# 控制文件存储类型
a1.sinks.k1.hdfs.fileType = CompressedStream
a1.sinks.k1.hdfs.codeC = gzip
# 组装
a1.sources.r1.channels = c1
a1.sinks.k1.channel = c1
```
> Flume拦截器实现逻辑实现见模块`flume-interceptor`中`com.leon.gamll.flume.interceptor.TimestampAndTableNameInterceptor.class`

<b>测试</b>
需要的服务：hdfs、Zookeeper、Kafka、Maxwell
    <ol>
    <li>在bigdata103节点下`/opt/module/flume-1.9.0`目录中启动flume采集:</br>`./bin/flume-ng agent -n a1 -c conf/ -f jobs/gmall/kafka_to_hdfs_db.conf -Dflume.root.logger=info,console`</li>
    <li>在bigdata101节点`/opt/module/db_log`目录下执业务日志生成脚本:`java -jar gmall2020-mock-db-2021-11-14.jar`</li>
    <li>通过HDFS Web页面查看`/origin_data/gmall/db`相关目录下查看对应的文件是否生成</li>
    </ol>

编写一键启停脚本：`f3_db.sh` 

保存目录：`~/bin`

执行节点: bigdata101

```shell
#! /bin/bash
if [ $# -lt 1 ]
then
    echo "参数不能为空! 请输入start | stop"
    exit
fi

flumeconf=kafka_to_hdfs_db.conf
host=bigdata103

case $1 in
"start")
    ssh $host "nohup /opt/module/flume-1.9.0/bin/flume-ng agent -n a1 -c /conf -f /opt/module/flume-1.9.0/jobs/gmall/$flumeconf >/dev/null 2>&1 &"
    echo "$host 下游业务数据采集服务启动"
;;
"stop")
    ssh $host "ps -ef | grep -v 'grep' | grep -i $flumeconf | awk '{print \$2}' | xargs -n1 kill -9"
    echo "$host 下游业务数据采集服务已停止"
;;
*)
    echo "输入的参数有误! 请输入 start | stop"
;;
esac
```

测试环境可能出现的问题(<b>!!!生产环境不会产生此问题!!!</b>)：

HDFS上文件夹的日期与业务数据的产生时间不同

原因：

Maxwell在采集MySQL的BinLog数据时生成的JSON字符串中的ts记录的是操作时的系统日期

解决:

测试环境搭建的Maxwell是修改版，添加了mock_date配置，可以在Maxwell的`config.properties`配置添加
```properties
mock_date=2020-06-14
```
之后重启Maxwell重新采集数据即可

#### 增量数据同步之首日全量导入
由于Maxwell只能采集其运行期间MySQL数据库发生的变化，之前的数据无法采集。但是Maxwell提供了全量导入的方式--`maxwell-bootstrap`
，使用该命令可以将之前的数据进行同步

编写增量数据首日同步脚本 `mysql_to_hdfs_inc_init.sh`

位置: `~\bin`
```shell
#!/bin/bash

if [ $# -lt 1 ]
then
    echo "参数能为空! 请输入要同步的增量表名，同步所有增量表请输入 all "
fi

MAXWELL_HOME=/opt/module/maxwell

import_data() {
  $MAXWELL_HOME/bin/maxwell-bootstrap --database gmall --table $1 --config $MAXWELL_HOME/config.properties
}

case $1 in
"cart_info")
  import_data cart_info
;;
"coupon_use")
  import_data coupon_use
;;
"comment_info")
  import_data comment_info
;;
"favor_info")
  import_data favor_info
;;
"order_detail")
  import_data order_detail
;;
"order_detail_activity")
  import_data order_detail_activity
;;
"order_detail_coupon")
  import_data order_detail_coupon
;;
"order_info")
  import_data order_info
;;
"order_refund_info")
  import_data order_refund_info
;;
"order_status_log")
  import_data order_status_log
;;
"payment_info")
  import_data payment_info
;;
"refund_payment")
  import_data refund_payment
;;
"user_info")
  import_data user_info
;;
"all")
import_data cart_info
import_data coupon_use
import_data comment_info
import_data favor_info
import_data order_detail
import_data order_detail_activity
import_data order_detail_coupon
import_data order_info
import_data order_refund_info
import_data order_status_log
import_data payment_info
import_data refund_payment
import_data user_info
;;
esac
```

## 1.4 采集阶段总结
需要的服务：`HDFS`、`Zookeeper`、`Kafka`、`Maxwell`

<b>用户行为日志采集脚本调用路线</b>

日志生成模块: `lg.sh`→上游数据采集: `f1_log.sh`→下游数据采集: `f2_log.sh`→`HDFS`

<b>业务日志采集脚本调用路线</b>

业务生成模块:

`java -jar gmall2020...`→ (首次调用需生成dataX的JSON配置文件:`gen_import_config.sh`→)全量数据同步 [dataX]: `mysql_to_hdfs_full.sh`→`HDFS`

业务生成模块: 

`java -jar gmall2020...`→ (增量数据首日同步 [Maxwell]: `mysql_to_hdfs_inc_init.sh`→)增量数据同步 [Maxwell]: `f3_db.sh`→`HDFS`